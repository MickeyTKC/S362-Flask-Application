# Full name and 8-digit student number
- Name: Tse Kai Chun
- Student number: 12755669

# Files
- `legacy_pi_server.Py`: Legacy server code uses primitive π calculation technology.
- `s12755669_server.Py`: API Flask server code that provides endpoints for π calculation and user statistics.
- `s12755669_test.Py`: Unit test code used to verify Flask server functionality.
- `s12755669_client.Py`: Testing specific functionality and output from interaction with the server.

# Instructions for putting in place server application
### Libraries
- `os`: For interacting with the running machine.
- `time` and `timeit`: For measuring execution time.
- `Flask`: A web framework for constructing the API.
- `random`: For generating random numbers inside the Monte Carlo simulation.
- `threading.Lock()`: For coping with concurrent connections within the Statistics carrier (Read File, Write File).
- `concurrent.Futures`: For using `ProcessPoolExecutor` and `ThreadPoolExecutor`.
- `socket`: For handling TCP and UDP connections.
- `regex`: For advanced string matching and validation.

# Instructions for putting in take a look at application
### Libraries
- `os`: For running machine interactions.
- `subprocess`: For executing subprocesses.
- `unittest`: For structuring and strolling assessments.
- `time`: For measuring time throughout assessments.
- `json`: For handling JSON statistics in requests and responses.
- `requests`: For making HTTP requests to the API.


# JSON format of the Statistics web service 
- Example: 
```json
[
  {"request_count": 10, "username": "1111"},
  {"request_count": 10, "username": "2222"}
]
```

# Description and Justification of the Concurrency Solutions in the Pi and Legacy Pi Web Services

The Pi and Legacy Pi services in `s12755669_server.py` utilize concurrency to efficiently calculate π (Pi) using different methods, enhancing performance and user experience by allowing the server to handle multiple requests simultaneously.

## 1. Pi Service

The Pi service implements the Monte Carlo method to calculate π. Key features of its concurrent solutions include:

- **Process Pooling** 
  - Uses `ProcessPoolExecutor` to distribute workload across multiple processes, leveraging multi-core processors.

- **Scalability**: 
  - Manages increased requests without significant response time delays by allocating more processes as needed.

- **Responsiveness**: 
  - Reduces execution time for high simulation counts, providing quicker results to users.

- **Error Handling**: 
  - Allows graceful management of failures in individual processes, enhancing overall system reliability.

### Implementation Overview

When a request is obtained at the `/pi` endpoint:
- Input parameters are validated.
- Total simulations are divided by the required concurrency degree.
- Simulations are run in parallel the usage of `ProcessPoolExecutor`.
- Results are aggregated to provide the final π estimate.

## 2. Legacy Pi Service

Legacy Pi services use traditional network protocols (TCP and UDP) to calculate π. Its concurrency solutions focus on efficiently handling multiple network requests:

- **Protocol Support**: 
  - Supports both TCP and UDP, allowing flexibility in how to query π calculations.

- **Process Pooling:** 
  - Uses `ProcessPoolExecutor` to distribute workloads across multiple processes, taking advantage of multi-core processors.

- **Responsiveness**: 
  - Provides fast responses for customers, regardless of the protocol used, maintaining low latency.

- **Load Balancing**: 
  - Distribute requests to available threads, optimizing resource utilization and improving throughput.

### Implementation Overview

When a request is made to the `/legacy_pi` endpoint:
- The server determines the protocol (TCP or UDP).
- For TCP, a connection is established, and records is despatched/received the use of sockets.
- For UDP, data is despatched with out setting up a connection.
- Responses are generated based totally at the calculations finished in parallel.

# Discussion on Adopting Advanced Technologies in Server Programming

## Introduction
Hiring the best technology is miles important to increase server performance and scalability. The information below is devoted to the basic concepts of asynchronous programming, high-performance computing, and research methods.

## 1. Asynchronous Programming
Asynchronous programming is essential for reactive server applications, that allow them handle multiple requests together.

- **Non-blocking I/O:**
  - Libraries like `asyncio` permit servers to manipulate many connections simultaneously with out watching for I/O operations to finish. This permits the server to stay responsive and correctly handle a couple of requests.

- **Resource Efficiency:**
  - Tasks can yield for the duration of I/O operations. This allows the server to manage other requests. This better resource utilization, as the CPU is not left idle while looking forward to I/O operations.

- **High Concurrency:**
  - An asynchronous programming server can handle more concurrent connections than the traditional threading model, where each connection may require its thread. This is especially beneficial for applications with large numbers of concurrent clients, including chat programs or streaming services.

- **Reduced Latency:**
  - By handling I/O operations asynchronously, the program can achieve lower response times. Users experience faster interactions because the server can now respond to new requests even while ongoing requests are still being processed.

- **Scalability:**
  - Asynchronous programs scale more successfully because they require fewer system assets (such as threads) to control large numbers of connections. This makes it easier to scale applications to address faster access without increasing hardware assets.
  
- **Simplified Code Flow:**
  - Although asynchronous code can to begin with appear complicated, the use of `async` and `look forward to` key phrases permits builders to jot down code that reads sequentially, making it less complicated to apprehend the glide of good judgment in comparison to conventional callback-heavy techniques.

## 2. High-Performance Computing (HPC)
HPC techniques improve performance of data-intensive server applications.

- **Native Code Libraries:** Use libraries like NumPy for efficient numerical calculations.
- **Native Modules:** Tools like Cython can compile critical Python code into native code for speed.
- **Concurrency Models:** Multiprocessing and MPI support allocate workloads across CPUs or machines.

## 3. Profiling and Optimization
Profiling helps identify performance bottlenecks in server applications.

- **Tools:** Create detailed performance reports with cProfile.
- **Best Practices:** Make sure you do it right before you do it right, and focus on the things that take the most time.

## 4. Message Passing in Distributed Servers
The message interface (MPI) is essential for communication in distributed server architectures.

- **SPMD Model:** Allows multiple methods to execute the same operation with different data sets, improving computational efficiency.
- **Integration with Python:** The `mpi4py` library facilitates efficient data communication between systems.

## Conclusion
We can make server packages more powerful by using asynchronous programming, HPC techniques, and profiling techniques. Developers can create scalable and efficient solutions that meet user needs through functionality.